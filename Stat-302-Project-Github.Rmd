---
title: "Stat-302 Project (GitHub)"
author: "Miles Matthews"
date: "2024-01-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
#library(gmodels)
#library(Sleuth3)
#library(DescTools)
```

### Importing data

```{r}
CollegeDistance <- read.csv("CollegeDistance.csv")
```

## Some two way relationships between variables

### Gender and distance:

#### Checking variables

Checking what variables are in gender

```{r}
unique(CollegeDistance$gender)
```

Since the relationship between a categorical variable and a quantitative variable is being examined, boxplots and histograms can be good ways to investigate this data.

#### Boxplot:

```{r}
ggplot(data = CollegeDistance, mapping = aes(gender, distance))+
  geom_boxplot()
```

Given that the data for distance appears skewed, I will try transforming it from now on.

#### Transformed boxplot:

```{r}
ggplot(data = CollegeDistance, mapping = aes(gender, log(distance)))+
  geom_boxplot()
```

It appears that the relationship between distance and gender is roughly the same for both men and women. The tail for women seems to contain larger values for distance than the tail for men, potentially indicating a gender disparity among the students who have the least access to a nearby college.

### Income and distance:

Checking what variables are present in income.

```{r}
unique(CollegeDistance$income)
```

Since a relationship between a categorical variable and quantitative variable is being examined, boxplots can be a good way to investigate this data.

#### Transformed boxplot:

```{r}
ggplot(data = CollegeDistance, mapping = aes(income, log(distance)))+
  geom_boxplot()
```

Once again, the relationships between income and distance look roughly the same between different income levels, however the distance for low income students appears to have more variability. There might be one outlier in the log of distance for high income students.

## Multiple linear regression analysis:

Here I am preparing distance data by adding a new variable to college distance with the transformed distance data.

```{r}
CollegeDistance2 <- CollegeDistance |>
  mutate(log_distance = log(distance+0.1))

```

### Full versus reduced model

Examining full model to choose variables to include in reduced model. In the reduced model, I will include the variables that R marks with one or more stars to indicate significant p values.

```{r}

mlr1 = lm(score ~ ., CollegeDistance2)
summary(mlr1)


mlr2 = lm(score ~ gender + ethnicity + fcollege + mcollege + home + urban + unemp + wage + tuition + education + log_distance, CollegeDistance2)
summary(mlr2)
```

#### Testing to make sure that the reduced model explains score roughly as well as the full model

Gathering anova data

```{r}
anova(mlr1)
anova(mlr2)
```

Assigning values to relevant parts of anova data:

```{r}
full_SSE <- 237079
reduced_SSE <- 237540 
ExtraSS <- reduced_SSE - full_SSE
n <- 4738
p <- 16
q <- 12
```

Calculating F statistic:

```{r}
F_stat <- (ExtraSS / (p-q)) / (full_SSE / (n-p))
F_stat
```

Calculating p value:

```{r}
anova(mlr1,mlr2)
```

The p value (0.05692) is greater than 0.05, so we fail to reject that the reduced model is roughly as accurate as the full one for predicting score. However, it is barely over 0.05, we can proceed with the reduced model but we should proceed with caution.

Credit to <https://www.statology.org/how-to-calculate-the-p-value-of-an-f-statistic-in-r/> for reference.

### Stating assumptions:

1.  Linearity
2.  Independence of errors
3.  Independence of independent variables
4.  Homoskedasticity: similar variances for each group
5.  Normality

Credit to <https://www.cfainstitute.org/en/membership/professional-development/refresher-readings/multiple-regression#:~:text=Five%20main%20assumptions%20underlying%20multiple,whether%20these%20assumptions%20are%20satisfied.> for reference.

### Does the data meet the assumptions?

#### Residual plots 1: scatterplot of the residuals versus explanatory variable

Note: I will only be checking the numerical explanatory variables (wage, tuition, education, and log of distance), as recommended by Dr. Kacar.

Plot for wage:

```{r}
ggplot(mlr2, aes(x = CollegeDistance2$wage, y = .resid)) + 
  geom_point(color = "dark blue") +
  geom_hline(yintercept = 0) +
  xlab("Wage") +
  ylab("Residuals") +
  ggtitle("Residuals vs wage")+
  theme_bw()
```

The residuals seem evenly centered around zero with no concerning patterns, so we can assume that the data for wages meets the linearity condition.

Plot for tuition:

```{r}
ggplot(mlr2, aes(x = CollegeDistance2$tuition, y = .resid)) + 
  geom_point(color = "dark blue") +
  geom_hline(yintercept = 0) +
  xlab("Tuition") +
  ylab("Residuals") +
  ggtitle("Plot of residuals vs tuition")+
  theme_bw()
```

The residuals seem evenly centered around zero with no concerning patterns, so we can assume that the data for tuition meets the linearity condition.

Plot for education:

```{r}
ggplot(mlr2, aes(x = CollegeDistance2$education, y = .resid)) + 
  geom_point(color = "dark blue") +
  geom_hline(yintercept = 0) +
  xlab("Number of years of education") +
  ylab("Residuals") +
  ggtitle("Plot of residuals vs years of education")+
  theme_bw()
```

The residuals seem roughly evenly centered around zero, however there does seem to be a downward trend in residuals as number of years of education increases. So, we should be cautious when assuming that the data in the model is linear for number of years of education. However, I don't think that it is so concerning that we can't continue with the test.

Plot for log of distance:

```{r}
ggplot(mlr2, aes(x = CollegeDistance2$log_distance, y = .resid)) + 
  geom_point(color = "dark blue") +
  geom_hline(yintercept = 0) +
  xlab("Log of distance from nearest college") +
  ylab("Residuals") +
  ggtitle("Plot of residuals vs log of distance")+
  theme_bw()
```

The residuals seem roughly evenly centered around zero. However they do seem condensed into the right side of the x axis. So, we should be cautious when assuming that the data in the model is linear for log of distance. However, I don't think that it is so concerning that we can't continue with the test.

#### Residual plot 2: scatterplot of the residuals versus fitted values

```{r}

ggplot(mlr2, aes(x = .fitted, y = .resid)) + 
  geom_point(color = "dark blue") +
  geom_hline(yintercept = 0) +
  xlab("Fitted Values") +
  ylab("Residuals") +
  ggtitle("Plot of residuals vs fitted values: Student information versus student success")+
  theme_bw()
```

The residuals seem roughly evenly centered around zero with no concerning patterns, so we can assume that the data for the model meets the homoscedasticity condition.

#### Residual plot 3: Q-Q plot of the residuals

```{r}
ggplot(mlr2, mapping = aes(sample = mlr2$residuals)) + 
  stat_qq() +
  stat_qq_line() +
  geom_qq( color = "dark red") +
  ggtitle("qq plot of Residuals: Demographic information vs distance") +
  ylab("Residuals") +
  xlab("N(0,1) quantiles") +
  theme_bw()
```

The tails of the qq plot diverge somewhat from the line, showing that there may be some non-normality in the data. However, since there is a very large number of students, we can apply CLT. So, we can assume that the data for the model meets the normality condition.

Credit to <https://thestatsgeek.com/2013/08/07/assumptions-for-linear-regression/> for reference.

Credit to <https://www.statology.org/multiple-linear-regression-assumptions/> and to <https://www.r-bloggers.com/2020/10/residual-plots-and-assumption-checking/#> for reference.

## Running the test:

Here is the original reduced model:

```{r}
mlr2 = lm(score ~ gender + ethnicity + fcollege + mcollege + home + urban + unemp + wage + tuition + education + log_distance, CollegeDistance2)
summary(mlr2)
```

All of the variables are significant except for unemployment. I will make a new reduced model that does not contain unemployment and test it against the original reduced model.

```{r}
mlr3 = lm(score ~ gender + ethnicity + fcollege + mcollege + home + urban  + wage + tuition + education + log_distance, CollegeDistance2)
summary(mlr3)
```

Now I will run the anova test to make sure that the model without employment has not lost much accuracy.

```{r}
anova(mlr2)
anova(mlr3)
```

Assigning the Anova output to the different components of the F test:

```{r}
SSE1 <- 237540
SSE2 <- 237687 
ExtraSS2 <- SSE2 - SSE1
n <- 4738
p2 <- 12
q2 <- 11
```

Calculating the F statistic:

```{r}
F_stat2 <- (ExtraSS2 / (p2-q2)) / (SSE1 / (n-p2))
F_stat2
```

Calculating the p value:

```{r}
anova(mlr2, mlr3)
```

Since the p value (0.08734) is greater than 0.05, we fail to reject the claim that the original reduced model is much more accurate than the model without income. So, we can go ahead and drop income from our model.

Here is our final model and summary statistics:

```{r}
summary(mlr3)
```

As you can see every variable is now statistically significant!

## Conclusion and Interpretation:

The mlr test gave important information about how various factors influence student's success. Unsurprisingly, the variable with the largest t score was education, with a t score of 31.46. This indicates that having more years of education greatly improves students test scores. However, the next most impactful variable was ethnicity. Strangely, the dataset only included three categories for ethnicity: African American, Hispanic, and Other.

```{r}
unique(CollegeDistance2$ethnicity)
```

Presumably many people in the "Other" category would be classified as White if the researchers had collected that data. Other had a t score of 21.43, indicating a tremendous advantage. Hispanic had a smaller, but still significant t score of 7.635, indicating that African Americans are most disadvantaged when it comes to student scores, but Hispanic students are also heavily disadvantaged against White students. This speaks to the deep racial inequities in our education system and society.

The next largest t score was gender, with a t score of 5.25. Like race, this speaks to the gender disparities in our education system and society. Additionally, the fact that the second and third greatest determinant of student success were race and gender, respectively, speaks to the continued work that needs to be done to ensure that there truly is liberty and justice for all.

Other factors that influence student success, in order of magnitude, are the increase in a students scores if their mother went to college (t score 4.19), the decrease in their scores if their distance from college is greater (t score -4.15), the increase their scores if their father went to college (t score 3.04), the increase in their scores if their average state 4-year college tuition was higher (t score 2.87), the increase in their scores if their family owns their own home (t score 2.57), the increase in their scores if the manufacturing wage in their state is higher (t score 2.47), and the decrease in their scores if the student lives in an urban area (t score of -2.15). The p value for unemployment (0.087) was greater than 0.05, so I wouldn't say it had a very significant impact.

The model also has an extremely high F statistic of 201, indicating that it is extremely likely that there is a relationship between the combination of influences and student test scores.

However, the model doesn't explain very much of the variability in the data. It has a low Multiple R-squared value of 0.338. In other words, it doesn't capture many of the variables that impact student success. However, it can still be useful in examining the impacts that it does cover.

This data indicates that the biggest thing that people can do to boost student achievement may be to help students access more years of education, and tackle racial and gender disparities. While this data was collected in 1980, there are still striking racial disparities in education access to this day (<https://home.treasury.gov/news/featured-stories/post-5-racial-differences-in-educational-experiences-and-attainment>). In addition, the educational disparities in 1980 may have made it more difficult for Black and Hispanic families to build generational wealth during this time compared to White families. Adressing gender inequalities may also significantly improve students test scores. Additionally, examining and addressing the impact that factors in the model such as mother's college degree obtainment, distance from nearest college, and fathers college obtainment have on students may also increase student performance.
